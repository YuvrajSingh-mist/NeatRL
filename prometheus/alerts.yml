groups:
  - name: service-health
    rules:
      - alert: AnyTargetDown
        expr: up == 0
        for: 15s
        labels:
          severity: critical
        annotations:
          summary: A metrics target is down
          description: 'Job {{ $labels.job }} instance {{ $labels.instance }} is not scrapeable.'

      - alert: APITargetDown
        expr: up{job="api"} == 0
        for: 20s
        labels:
          severity: critical
        annotations:
          summary: API target is down
          description: Prometheus cannot scrape the API /metrics endpoint.

      - alert: WorkerTargetDown
        expr: up{job="celery_worker"} == 0
        for: 20s
        labels:
          severity: critical
        annotations:
          summary: Celery worker metrics target is down
          description: Prometheus cannot scrape the worker metrics endpoint.

  - name: rl-quality
    rules:
      - alert: HighEvaluationFailures
        expr: increase(evaluation_failed_total[1m]) > 3
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: High evaluation failure rate
          description: More than 3 failed evaluations in the last minute.

      - alert: SlowEvaluationsP95
        expr: histogram_quantile(0.95, sum(rate(evaluation_duration_seconds_bucket[1m])) by (le)) > 30
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Evaluation p95 latency is high
          description: p95 evaluation duration exceeds 30 seconds.

      - alert: LeaderboardLatencyHigh
        expr: rate(leaderboard_query_duration_seconds_sum[1m]) / rate(leaderboard_query_duration_seconds_count[1m]) > 0.5
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Leaderboard query latency is high
          description: Average leaderboard query duration exceeds 500ms over 10 minutes.

      - alert: CeleryQueueBacklog
        expr: celery_queue_length{queue_name="celery"} > 20
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Celery queue backlog
          description: Celery queue length is above 20 for 2 minutes.

  - name: infrastructure
    rules:
      - alert: BlackboxProbeFailCritical
        expr: probe_success{job="blackbox_http"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: Blackbox probe failed (critical target)
          description: 'HTTP probe failed for critical target {{ $labels.instance }}.'

      - alert: BlackboxProbeFailWarning
        expr: probe_success{job="blackbox_http_aux"} == 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: Blackbox probe failed (auxiliary target)
          description: 'HTTP probe failed for auxiliary target {{ $labels.instance }}. Investigate if persistent.'

      - alert: APIHealthCheckFail
        expr: probe_success{instance="http://api:8000/health"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: API health check is failing
          description: 'API health endpoint is not responding properly. Check API logs and worker status.'

      - alert: RedisExporterTargetDown
        expr: up{job="redis"} == 0
        for: 40s
        labels:
          severity: critical
        annotations:
          summary: Redis exporter target is down
          description: Prometheus cannot scrape the redis_exporter metrics target.

      - alert: RedisDown
        expr: redis_up == 0
        for: 40s
        labels:
          severity: critical
        annotations:
          summary: Redis is down
          description: redis_exporter reports the Redis instance is unreachable.

      - alert: DatabaseDown
        expr: database_health == 0
        for: 20s
        labels:
          severity: critical
        annotations:
          summary: Database connection is down
          description: Database health check failed for 30 seconds. This will prevent task processing and leaderboard updates.

      - alert: DatabaseConnectionError
        expr: database_health == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: Database connection error persisting
          description: Database has been down for 5 minutes. Immediate attention required - tasks cannot be processed.

      - alert: RedisConnectionDown
        expr: redis_health == 0
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: Redis connection is down
          description: Redis health check failed for 30 seconds. This will affect leaderboard and task processing.

      - alert: RedisConnectionError
        expr: redis_health == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: Redis connection error persisting
          description: Redis has been down for 5 minutes. Leaderboard and task processing will be affected.

      - alert: CeleryWorkerDown
        expr: celery_worker_health == 0
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: Celery workers are down
          description: No Celery workers responding for 30 seconds. Task processing has stopped.

      - alert: CeleryWorkerError
        expr: celery_worker_health == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: Celery worker error persisting
          description: Celery workers have been down for 5 minutes. No tasks can be processed.

      - alert: CeleryWorkerOverload
        expr: celery_reserved_tasks > 10
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Celery workers are overloaded
          description: More than 10 tasks are waiting to be processed. Workers may be overloaded or stuck.

      - alert: CeleryWorkerStuck
        expr: celery_active_tasks > 5
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Celery workers may be stuck
          description: More than 5 tasks have been active for 5 minutes. Workers may be stuck processing tasks.

      - alert: SupabaseStorageDown
        expr: supabase_storage_health == 0
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: Supabase storage is down
          description: Supabase storage health check failed for 30 seconds. File uploads/downloads will fail.

      - alert: SupabaseStorageError
        expr: supabase_storage_health == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: Supabase storage error persisting
          description: Supabase storage has been down for 5 minutes. File operations will fail.

      - alert: OverallSystemDown
        expr: overall_system_health == 0
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: Overall system health check failed
          description: One or more critical system components are down. Immediate attention required.

      - alert: SystemHealthDegraded
        expr: overall_system_health == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: System health degraded for extended period
          description: System has been unhealthy for 5 minutes. Service quality is severely impacted.

      - alert: RedisMemoryHigh
        expr: redis_memory_max_bytes > 0 and (redis_memory_used_bytes / redis_memory_max_bytes) > 0.9
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Redis memory usage high
          description: Redis memory usage exceeded 90% of the configured maxmemory.

      - alert: RedisEvictions
        expr: increase(redis_evicted_keys_total[1m]) > 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: Redis is evicting keys
          description: Redis evicted keys in the last 5 minutes, likely due to memory pressure.

      - alert: HostCPUHigh
        expr: 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[1m])) * 100) > 90
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Host CPU usage is high
          description: Average CPU utilization across cores exceeded 90% for 10 minutes.

      - alert: HostMemoryHigh
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Host memory usage is high
          description: Host memory usage exceeded 90% for 10 minutes.

